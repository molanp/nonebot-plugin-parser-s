import asyncio
import json
import re
from typing import Any, ClassVar
from typing_extensions import override

from bilibili_api import HEADERS, Credential, request_settings, select_client
from bilibili_api.video import Video
from nonebot import logger

from ..config import plugin_cache_dir, plugin_config_dir, rconfig
from ..cookie import ck2dict
from ..download import DOWNLOADER
from ..exception import DownloadSizeLimitException, ParseException
from ..utils import merge_av
from .base import BaseParser
from .data import Content, ImageContent, Platform, TextImageContent, VideoContent
from .utils import get_redirect_url


class BilibiliParser(BaseParser):
    # å¹³å°ä¿¡æ¯
    platform: ClassVar[Platform] = Platform(name="bilibili", display_name="å“”å“©å“”å“©")

    # URL æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆkeyword, patternï¼‰
    patterns: ClassVar[list[tuple[str, str]]] = [
        ("bilibili", r"https?://(?:space|www|live|m|t)?\.?bilibili\.com/[A-Za-z\d\._?%&+\-=/#]+()()"),
        ("bili2233", r"https?://bili2233\.cn/[A-Za-z\d\._?%&+\-=/#]+()()"),
        ("b23", r"https?://b23\.tv/[A-Za-z\d\._?%&+\-=/#]+()()"),
        ("BV", r"(BV[1-9a-zA-Z]{10})(?:\s)?(\d{1,3})?"),
        ("av", r"av(\d{6,})(?:\s)?(\d{1,3})?"),
    ]

    def __init__(self):
        self.headers = HEADERS.copy()
        self._credential: Credential | None = None
        self._cookies_file = plugin_config_dir / "bilibili_cookies.json"
        # é€‰æ‹©å®¢æˆ·ç«¯
        select_client("curl_cffi")
        # æ¨¡ä»¿æµè§ˆå™¨
        request_settings.set("impersonate", "chrome131")
        # ç¬¬äºŒå‚æ•°æ•°å€¼å‚è€ƒ curl_cffi æ–‡æ¡£
        # https://curl-cffi.readthedocs.io/en/latest/impersonate.html

    @override
    async def parse(self, matched: re.Match[str]):
        """è§£æ URL è·å–å†…å®¹ä¿¡æ¯å¹¶ä¸‹è½½èµ„æº

        Args:
            matched: æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¯¹è±¡ï¼Œç”±å¹³å°å¯¹åº”çš„æ¨¡å¼åŒ¹é…å¾—åˆ°

        Returns:
            ParseResult: è§£æç»“æœï¼ˆå·²ä¸‹è½½èµ„æºï¼ŒåŒ…å« Pathï¼‰

        Raises:
            ParseException: è§£æå¤±è´¥æ—¶æŠ›å‡º
        """
        # ä»åŒ¹é…å¯¹è±¡ä¸­è·å–åŸå§‹URL, è§†é¢‘ID, é¡µç 
        url, video_id, page_num = str(matched.group(0)), str(matched.group(1)), matched.group(2)

        link = None
        # å¤„ç†çŸ­é“¾
        if "b23.tv" in url or "bili2233.cn" in url:
            link = url
            url = await get_redirect_url(url, self.headers)

        avid, bvid = None, None
        # é“¾æ¥ä¸­æ˜¯å¦åŒ…å«BVï¼Œavå·
        if video_id:
            if video_id.isdigit():
                avid = int(video_id)
                link = f"https://bilibili.com/av{avid}"
            else:
                bvid = video_id
                link = f"https://bilibili.com/video/{bvid}"
            if page_num is not None:
                link += f"?p={page_num}"
        else:
            if _matched := re.search(r"(BV[\dA-Za-z]{10})[^?]*?(?:\?[^#]*?p=(\d{1,3}))?", url):
                bvid = _matched.group(1)
                page_num = _matched.group(2)
            elif _matched := re.search(r"av(\d{6,})[^?]*?(?:\?[^#]*?p=(\d{1,3}))?", url):
                avid = int(_matched.group(1))
                page_num = _matched.group(2)
            else:
                return await self.parse_others(url)

        page_num = int(page_num) if page_num and page_num.isdigit() else 1

        # è§£æè§†é¢‘ä¿¡æ¯
        parser_result = await self.parse_video(bvid=bvid, avid=avid, page_num=page_num)
        if link is not None and parser_result.title:
            parser_result.title += f"\n{link}"
        return parser_result

    async def parse_video(
        self,
        *,
        bvid: str | None = None,
        avid: int | None = None,
        page_num: int = 1,
    ):
        """è§£æè§†é¢‘ä¿¡æ¯

        Args:
            bvid (str | None): bvid
            avid (int | None): avid
            page_num (int): é¡µç 
        """

        video = await self._parse_video(bvid=bvid, avid=avid)
        video_info: dict[str, Any] = await video.get_info()

        video_duration: int = int(video_info["duration"])
        cover_url: str | None = None
        title: str = video_info["title"]

        # å¤„ç†åˆ† p
        page_idx = page_num - 1
        if (pages := video_info.get("pages")) and len(pages) > 1:
            assert isinstance(pages, list)
            # å–æ¨¡é˜²æ­¢æ•°ç»„è¶Šç•Œ
            page_idx = page_idx % len(pages)
            p_video = pages[page_idx]
            # è·å–åˆ†é›†æ—¶é•¿
            video_duration = int(p_video.get("duration", video_duration))
            # è·å–åˆ†é›†æ ‡é¢˜
            if p_name := p_video.get("part").strip():
                title += f"\nåˆ†é›†: {p_name}"
            # è·å–åˆ†é›†å°é¢
            if first_frame_url := p_video.get("first_frame"):
                cover_url = first_frame_url
        else:
            page_idx = 0

        # è·å–åœ¨çº¿è§‚çœ‹äººæ•°
        online = await video.get_online()
        display_info = (
            f"{self._extra_bili_info(video_info)}\n"
            f"ğŸ“ ç®€ä»‹ï¼š{video_info['desc']}\n"
            f"ğŸ„â€â™‚ï¸ {online['total']} äººæ­£åœ¨è§‚çœ‹ï¼Œ{online['count']} äººåœ¨ç½‘é¡µç«¯è§‚çœ‹"
        )

        # è·å– AI æ€»ç»“
        ai_summary: str = "å“”å“©å“”å“© cookie æœªé…ç½®æˆ–å¤±æ•ˆ, æ— æ³•ä½¿ç”¨ AI æ€»ç»“"
        if self._credential:
            cid = await video.get_cid(page_idx)
            ai_conclusion = await video.get_ai_conclusion(cid)
            ai_summary = ai_conclusion.get("model_result", {"summary": ""}).get("summary", "").strip()
            ai_summary = f"AIæ€»ç»“: {ai_summary}" if ai_summary else "è¯¥è§†é¢‘æš‚ä¸æ”¯æŒAIæ€»ç»“"

        # è·å–éŸ³è§†é¢‘ä¸‹è½½é“¾æ¥
        cover_url = cover_url if cover_url else video_info.get("pic")
        video_url, audio_url = await self.parse_video_download_url(video=video, page_index=page_idx)

        cover_path = None
        file_name = f"{bvid or avid}-{page_num}"
        video_path = plugin_cache_dir / f"{file_name}.mp4"
        extra_info = f"{display_info}\n{ai_summary}".strip()
        # ä¸‹è½½å°é¢
        if cover_url:
            cover_path = await DOWNLOADER.download_img(cover_url, ext_headers=self.headers)

        contents: list[Content] = []
        # ä¸‹è½½è§†é¢‘
        if not video_path.exists():
            # ä¸‹è½½è§†é¢‘å’ŒéŸ³é¢‘
            try:
                if audio_url is not None:
                    v_path, a_path = await asyncio.gather(
                        DOWNLOADER.streamd(video_url, file_name=f"{file_name}-video.m4s", ext_headers=self.headers),
                        DOWNLOADER.streamd(audio_url, file_name=f"{file_name}-audio.m4s", ext_headers=self.headers),
                    )
                    await merge_av(v_path=v_path, a_path=a_path, output_path=video_path)
                else:
                    video_path = await DOWNLOADER.streamd(
                        video_url, file_name=f"{file_name}.mp4", ext_headers=self.headers
                    )
            except DownloadSizeLimitException as e:
                contents.append(e.message)

        if video_path.exists():
            contents.append(VideoContent(video_path))

        extra = {}
        if cover_path:
            extra["cover_path"] = cover_path
        if extra_info:
            extra["info"] = extra_info

        return self.result(
            title=title,
            contents=contents,
            extra=extra,
        )

    async def parse_others(self, url: str):
        """è§£æå…¶ä»–ç±»å‹é“¾æ¥"""
        # åˆ¤æ–­é“¾æ¥ç±»å‹å¹¶è§£æ
        # 1. åŠ¨æ€/å›¾æ–‡ (opus)
        if "t.bilibili.com" in url or "/opus" in url:
            matched = re.search(r"/(\d+)", url)
            if not matched:
                raise ParseException("æ— æ•ˆçš„åŠ¨æ€é“¾æ¥")
            opus_id = int(matched.group(1))
            img_urls, text = await self.parse_opus(opus_id)

            # ä¸‹è½½å›¾ç‰‡
            contents: list[Content] = []
            contents.append(text)
            if img_urls:
                pic_paths = await DOWNLOADER.download_imgs_without_raise(img_urls, ext_headers=self.headers)
                contents.extend(ImageContent(path) for path in pic_paths)

            return self.result(title=f"åŠ¨æ€ {opus_id}", contents=contents)

        # 2. ç›´æ’­
        if "/live" in url:
            match_result = re.search(r"/(\d+)", url)
            if not match_result:
                raise ParseException("æ— æ•ˆçš„ç›´æ’­é“¾æ¥")
            room_id = int(match_result.group(1))
            title, cover, keyframe = await self.parse_live(room_id)

            # ä¸‹è½½å°é¢
            cover_path = None
            if cover:
                cover_path = await DOWNLOADER.download_img(cover, ext_headers=self.headers)

            contents = []

            # ä¸‹è½½å…³é”®å¸§
            if keyframe:
                keyframe_path = await DOWNLOADER.download_img(keyframe, ext_headers=self.headers)
                contents.append(ImageContent(keyframe_path))

            extra = {}
            if cover_path:
                extra["cover_path"] = cover_path

            return self.result(title=title, contents=contents, extra=extra)

        # 3. ä¸“æ 
        if "/read" in url:
            match_result = re.search(r"/cv(\d+)", url)
            if not match_result:
                raise ParseException("æ— æ•ˆçš„ä¸“æ é“¾æ¥")
            read_id = int(match_result.group(1))
            texts, img_urls = await self.parse_read(read_id)
            combined_text = "\n".join(texts)

            # ä¸‹è½½å›¾ç‰‡
            contents = []
            contents.append(combined_text)
            if img_urls:
                pic_paths = await DOWNLOADER.download_imgs_without_raise(img_urls, ext_headers=self.headers)
                contents.extend(ImageContent(path) for path in pic_paths)

            return self.result(contents=contents)

        # 4. æ”¶è—å¤¹
        if "/favlist" in url:
            match_result = re.search(r"fid=(\d+)", url)
            if not match_result:
                raise ParseException("æ— æ•ˆçš„æ”¶è—å¤¹é“¾æ¥")
            fav_id = int(match_result.group(1))
            titles, cover_urls = await self.parse_favlist(fav_id)

            # å¹¶å‘ä¸‹è½½å°é¢
            cover_paths = await DOWNLOADER.download_imgs_without_raise(cover_urls, ext_headers=self.headers)

            return self.result(
                title=f"æ”¶è—å¤¹: {fav_id}",
                contents=[TextImageContent(title, cover_path) for title, cover_path in zip(titles, cover_paths)],
            )

        raise ParseException("ä¸æ”¯æŒçš„ Bilibili é“¾æ¥")

    async def _init_credential(self) -> Credential | None:
        """åˆå§‹åŒ– bilibili api"""

        if not rconfig.r_bili_ck:
            logger.warning("æœªé…ç½® r_bili_ck, æ— æ³•ä½¿ç”¨å“”å“©å“”å“© AI æ€»ç»“, å¯èƒ½æ— æ³•è§£æ 720p ä»¥ä¸Šç”»è´¨è§†é¢‘")
            return None

        credential = Credential.from_cookies(ck2dict(rconfig.r_bili_ck))
        if not await credential.check_valid() and self._cookies_file.exists():
            logger.info(f"r_bili_ck å·²è¿‡æœŸ, å°è¯•ä» {self._cookies_file} åŠ è½½")
            credential = Credential.from_cookies(json.loads(self._cookies_file.read_text()))
        else:
            logger.info(f"r_bili_ck æœ‰æ•ˆ, ä¿å­˜åˆ° {self._cookies_file}")
            self._cookies_file.write_text(json.dumps(credential.get_cookies()))

        return credential

    @property
    async def credential(self) -> Credential | None:
        """è·å–å“”å“©å“”å“©ç™»å½•å‡­è¯"""

        if self._credential is None:
            self._credential = await self._init_credential()
            if self._credential is None:
                return None

        if not await self._credential.check_valid():
            logger.warning("å“”å“©å“”å“© cookies å·²è¿‡æœŸ, è¯·é‡æ–°é…ç½® r_bili_ck")
            return self._credential

        if await self._credential.check_refresh():
            logger.info("å“”å“©å“”å“© cookies éœ€è¦åˆ·æ–°")
            if self._credential.has_ac_time_value() and self._credential.has_bili_jct():
                await self._credential.refresh()
                logger.info(f"å“”å“©å“”å“© cookies åˆ·æ–°æˆåŠŸ, ä¿å­˜åˆ° {self._cookies_file}")
                self._cookies_file.write_text(json.dumps(self._credential.get_cookies()))
            else:
                logger.warning("å“”å“©å“”å“© cookies åˆ·æ–°éœ€è¦åŒ…å« SESSDATA, ac_time_value, bili_jct")

        return self._credential

    async def parse_opus(self, opus_id: int) -> tuple[list[str], str]:
        """è§£æåŠ¨æ€ä¿¡æ¯

        Args:
            opus_id (int): åŠ¨æ€ id

        Returns:
            tuple[list[str], str]: å›¾ç‰‡ url åˆ—è¡¨å’ŒåŠ¨æ€ä¿¡æ¯
        """
        from bilibili_api.opus import Opus

        opus = Opus(opus_id, await self.credential)
        opus_info = await opus.get_info()
        if not isinstance(opus_info, dict):
            raise ParseException("è·å–åŠ¨æ€ä¿¡æ¯å¤±è´¥")

        # è·å–å›¾ç‰‡ä¿¡æ¯
        urls = await opus.get_images_raw_info()
        urls = [url["url"] for url in urls]

        dynamic = opus.turn_to_dynamic()
        dynamic_info: dict[str, Any] = await dynamic.get_info()
        orig_text = (
            dynamic_info.get("item", {})
            .get("modules", {})
            .get("module_dynamic", {})
            .get("major", {})
            .get("opus", {})
            .get("summary", {})
            .get("rich_text_nodes", [{}])[0]
            .get("orig_text", "")
        )
        return urls, orig_text

    async def parse_live(self, room_id: int) -> tuple[str, str, str]:
        """è§£æç›´æ’­ä¿¡æ¯

        Args:
            room_id (int): ç›´æ’­ id

        Returns:
            tuple[str, str, str]: æ ‡é¢˜ã€å°é¢ã€å…³é”®å¸§
        """
        from bilibili_api.live import LiveRoom

        room = LiveRoom(room_display_id=room_id, credential=await self.credential)
        room_info: dict[str, Any] = (await room.get_room_info())["room_info"]
        title, cover, keyframe = (
            room_info["title"],
            room_info["cover"],
            room_info["keyframe"],
        )
        return (title, cover, keyframe)

    async def parse_read(self, read_id: int) -> tuple[list[str], list[str]]:
        """ä¸“æ è§£æ

        Args:
            read_id (int): ä¸“æ  id

        Returns:
            texts: list[str], urls: list[str]
        """
        from bilibili_api.article import Article

        ar = Article(read_id)

        # åŠ è½½å†…å®¹
        await ar.fetch_content()
        data = ar.json()

        def accumulate_text(node: dict):
            text = ""
            if "children" in node:
                for child in node["children"]:
                    text += accumulate_text(child) + " "
            if _text := node.get("text"):
                text += _text if isinstance(_text, str) else str(_text) + node["url"]
            return text

        urls: list[str] = []
        texts: list[str] = []
        for node in data.get("children", []):
            node_type = node.get("type")
            if node_type == "ImageNode":
                if img_url := node.get("url", "").strip():
                    urls.append(img_url)
                    # è¡¥ç©ºä¸²å ä½ç¬¦
                    texts.append("")
            elif node_type == "ParagraphNode":
                if text := accumulate_text(node).strip():
                    texts.append(text)
            elif node_type == "TextNode":
                if text := node.get("text", "").strip():
                    texts.append(text)
        return texts, urls

    async def parse_favlist(self, fav_id: int) -> tuple[list[str], list[str]]:
        """è§£ææ”¶è—å¤¹ä¿¡æ¯

        Args:
            fav_id (int): æ”¶è—å¤¹ id

        Returns:
            tuple[list[str], list[str]]: æ ‡é¢˜ã€å°é¢ã€ç®€ä»‹ã€é“¾æ¥
        """
        from bilibili_api.favorite_list import get_video_favorite_list_content

        fav_list: dict[str, Any] = await get_video_favorite_list_content(fav_id)
        if fav_list["medias"] is None:
            raise ParseException("æ”¶è—å¤¹å†…å®¹ä¸ºç©º, æˆ–è¢«é£æ§")
        # å–å‰ 50 ä¸ª
        medias_50: list[dict[str, Any]] = fav_list["medias"][:50]
        texts: list[str] = []
        urls: list[str] = []
        for fav in medias_50:
            title, cover, intro, link = (
                fav["title"],
                fav["cover"],
                fav["intro"],
                fav["link"],
            )
            matched = re.search(r"\d+", link)
            if not matched:
                continue
            avid = matched.group(0) if matched else ""
            urls.append(cover)
            texts.append(f"ğŸ§‰ æ ‡é¢˜ï¼š{title}\nğŸ“ ç®€ä»‹ï¼š{intro}\nğŸ”— é“¾æ¥ï¼š{link}\nhttps://bilibili.com/video/av{avid}")
        return texts, urls

    async def _parse_video(self, *, bvid: str | None = None, avid: int | None = None) -> Video:
        """è§£æè§†é¢‘ä¿¡æ¯

        Args:
            bvid (str | None): bvid
            avid (int | None): avid
        """
        if avid:
            return Video(aid=avid, credential=await self.credential)
        elif bvid:
            return Video(bvid=bvid, credential=await self.credential)
        else:
            raise ParseException("avid å’Œ bvid è‡³å°‘æŒ‡å®šä¸€é¡¹")

    async def parse_video_download_url(
        self,
        *,
        video: Video | None = None,
        bvid: str | None = None,
        avid: int | None = None,
        page_index: int = 0,
    ) -> tuple[str, str | None]:
        """è§£æè§†é¢‘ä¸‹è½½é“¾æ¥

        Args:
            bvid (str | None): bvid
            avid (int | None): avid
            page_index (int): é¡µç´¢å¼• = é¡µç  - 1
        """

        from bilibili_api.video import (
            AudioStreamDownloadURL,
            VideoDownloadURLDataDetecter,
            VideoQuality,
            VideoStreamDownloadURL,
        )

        if video is None:
            video = await self._parse_video(bvid=bvid, avid=avid)
        # è·å–ä¸‹è½½æ•°æ®
        download_url_data = await video.get_download_url(page_index=page_index)
        detecter = VideoDownloadURLDataDetecter(download_url_data)
        streams = detecter.detect_best_streams(
            video_max_quality=VideoQuality._1080P,
            codecs=rconfig.r_bili_video_codes,
            no_dolby_video=True,
            no_hdr=True,
        )
        video_stream = streams[0]
        if not isinstance(video_stream, VideoStreamDownloadURL):
            raise ParseException("æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘æµ")
        logger.debug(f"è§†é¢‘æµè´¨é‡: {video_stream.video_quality.name}, ç¼–ç : {video_stream.video_codecs}")
        audio_stream = streams[1]
        if not isinstance(audio_stream, AudioStreamDownloadURL):
            return video_stream.url, None
        logger.debug(f"éŸ³é¢‘æµè´¨é‡: {audio_stream.audio_quality.name}")
        return video_stream.url, audio_stream.url

    def _extra_bili_info(self, video_info: dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–è§†é¢‘ä¿¡æ¯
        """
        # è·å–è§†é¢‘ç»Ÿè®¡æ•°æ®
        video_state: dict[str, Any] = video_info["stat"]

        # å®šä¹‰éœ€è¦å±•ç¤ºçš„æ•°æ®åŠå…¶æ˜¾ç¤ºåç§°
        stats_mapping = [
            ("ğŸ‘", "like"),
            ("ğŸª™", "coin"),
            ("â­", "favorite"),
            ("â†©ï¸", "share"),
            ("ğŸ’¬", "reply"),
            ("ğŸ‘€", "view"),
            ("ğŸ’­", "danmaku"),
        ]

        # æ„å»ºç»“æœå­—ç¬¦ä¸²
        result_parts = []
        for display_name, stat_key in stats_mapping:
            value = video_state[stat_key]
            # æ•°å€¼è¶…è¿‡10000æ—¶è½¬æ¢ä¸ºä¸‡ä¸ºå•ä½
            formatted_value = f"{value / 10000:.1f}ä¸‡" if value > 10000 else str(value)
            result_parts.append(f"{display_name} {formatted_value}")

        return " ".join(result_parts)
